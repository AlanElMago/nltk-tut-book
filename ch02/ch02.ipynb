{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Accessing Text Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corpora:** large bodies of linguistic data.\n",
    "\n",
    "**Text corpus:** large body of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Gutenberg Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK includes a small selection of texts from the Project Gutenberg electronic text archive.\n",
    "\n",
    "The `fileids()` function of the `corpus.gutenberg` package returns the file identifiers of the Gutenberg corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg # import the gutenberg package from the corpus package of the nltk library\n",
    "gutenberg.fileids()               # return the gutenberg file identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `words(\"`*`file.txt`*`\")` function of the `corpus.gutenberg` package returns all the tokens from a Gutenberg *text file*.\n",
    "\n",
    "The `raw(\"`*`file.txt`*`\")` function of the `corpus.gutenberg` package returns all the characters (a string of unprocessed text, or raw text) from a Gutenberg *text file*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192427\n",
      "887071\n"
     ]
    }
   ],
   "source": [
    "emma = gutenberg.words(\"austen-emma.txt\")   # store a list of tokens of the austen-emma.txt file\n",
    "print(len(emma))                            # print the total amount of tokens obtained from the austen-emma.txt file\n",
    "emma_raw = gutenberg.raw(\"austen-emma.txt\") # store all the characters (raw text) of the austen-emma.txt file\n",
    "print(len(emma_raw))                        # print the total amount of characters of the austen-emma.txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, the program loops through all the Gutenberg field identifiers and exctrats all the tokens that are words in each Gutenberg text. Statistics are shown for each text and are displayed in subsequent order:\n",
    "\n",
    "1. The average word length\n",
    "\n",
    "2. The average sentence length\n",
    "\n",
    "3. The average number of times each vocabulary item appears in the text\n",
    "\n",
    "4. The file identifier of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 21 23 austen-emma.txt\n",
      "4 22 15 austen-persuasion.txt\n",
      "4 24 19 austen-sense.txt\n",
      "4 26 63 bible-kjv.txt\n",
      "4 16  5 blake-poems.txt\n",
      "4 16 12 bryant-stories.txt\n",
      "4 15 11 burgess-busterbrown.txt\n",
      "4 16 11 carroll-alice.txt\n",
      "4 17 10 chesterton-ball.txt\n",
      "4 19  9 chesterton-brown.txt\n",
      "4 16  9 chesterton-thursday.txt\n",
      "4 17 21 edgeworth-parents.txt\n",
      "4 22 13 melville-moby_dick.txt\n",
      "4 43  9 milton-paradise.txt\n",
      "4 10  7 shakespeare-caesar.txt\n",
      "4 10  6 shakespeare-hamlet.txt\n",
      "4 10  5 shakespeare-macbeth.txt\n",
      "4 30 10 whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    words = [token for token in gutenberg.words(fileid) if token.isalpha()]\n",
    "    word_count = len(words)\n",
    "    word_lengths_sum = sum(len(word) for word in words)\n",
    "    sentence_count = len(gutenberg.sents(fileid))\n",
    "    vocabulary_count = len(set(word.upper() for word in words))\n",
    "\n",
    "    average_word_length = round(word_lengths_sum / word_count)\n",
    "    average_sentence_length = round(word_count / sentence_count)\n",
    "    average_vocabulary_occurrence_count = round(word_count / vocabulary_count)\n",
    "\n",
    "    print(f\"{average_word_length:1} {average_sentence_length:2} {average_vocabulary_occurrence_count:2} {fileid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sents(\"`*`file.txt`*`\")` function of the `corpus.gutenberg` package returns a list of sentences from a Gutenberg *text file*. Each sentence is a just list of tokens that end with an ending punctuation mark token. To put it simply: the list of sentences is just a list of lists that contain tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']'], ['Actus', 'Primus', '.'], ...]\n",
      "['Double', ',', 'double', ',', 'toile', 'and', 'trouble', ';', 'Fire', 'burne', ',', 'and', 'Cauldron', 'bubble']\n",
      "158\n",
      "[['Doubtfull', 'it', 'stood', ',', 'As', 'two', 'spent', 'Swimmers', ',', 'that', 'doe', 'cling', 'together', ',', 'And', 'choake', 'their', 'Art', ':', 'The', 'mercilesse', 'Macdonwald', '(', 'Worthie', 'to', 'be', 'a', 'Rebell', ',', 'for', 'to', 'that', 'The', 'multiplying', 'Villanies', 'of', 'Nature', 'Doe', 'swarme', 'vpon', 'him', ')', 'from', 'the', 'Westerne', 'Isles', 'Of', 'Kernes', 'and', 'Gallowgrosses', 'is', 'supply', \"'\", 'd', ',', 'And', 'Fortune', 'on', 'his', 'damned', 'Quarry', 'smiling', ',', 'Shew', \"'\", 'd', 'like', 'a', 'Rebells', 'Whore', ':', 'but', 'all', \"'\", 's', 'too', 'weake', ':', 'For', 'braue', 'Macbeth', '(', 'well', 'hee', 'deserues', 'that', 'Name', ')', 'Disdayning', 'Fortune', ',', 'with', 'his', 'brandisht', 'Steele', ',', 'Which', 'smoak', \"'\", 'd', 'with', 'bloody', 'execution', '(', 'Like', 'Valours', 'Minion', ')', 'caru', \"'\", 'd', 'out', 'his', 'passage', ',', 'Till', 'hee', 'fac', \"'\", 'd', 'the', 'Slaue', ':', 'Which', 'neu', \"'\", 'r', 'shooke', 'hands', ',', 'nor', 'bad', 'farwell', 'to', 'him', ',', 'Till', 'he', 'vnseam', \"'\", 'd', 'him', 'from', 'the', 'Naue', 'toth', \"'\", 'Chops', ',', 'And', 'fix', \"'\", 'd', 'his', 'Head', 'vpon', 'our', 'Battlements']]\n"
     ]
    }
   ],
   "source": [
    "# store the list sentences of the shakespeare-macbeth.txt file\n",
    "macbeth_sentences = gutenberg.sents(\"shakespeare-macbeth.txt\")\n",
    "\n",
    "# print the list of sentences of the text\n",
    "print(macbeth_sentences)\n",
    "\n",
    "# print the 1116th sentence of the text\n",
    "print(macbeth_sentences[1116])\n",
    "\n",
    "# store the length of the longest sentence of the text\n",
    "longest_sentence_length = max(len(sentence) for sentence in macbeth_sentences)\n",
    "\n",
    "# print the length of the longest sentence of the text\n",
    "print(longest_sentence_length)\n",
    "\n",
    "# print all the sentences that have the same length as the longest sentence in the text\n",
    "print([sentence for sentence in macbeth_sentences if len(sentence) == longest_sentence_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Web and Chat Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has a small cllection of web text that includes content from a Firefox discussion forum, conversations overheard in New York, the movie script of Pirates of the Carribean, personal advertisements, and wine reviews.\n",
    "\n",
    "The `fileids()` function of the `corpus.webtext` package returns the file identifiers of the web text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firefox.txt   Cookie Manager: \"Don't allow site ...\n",
      "grail.txt     SCENE 1: [wind] [clop clop clop]  ...\n",
      "overheard.txt White guy: So, do you have any pl ...\n",
      "pirates.txt   PIRATES OF THE CARRIBEAN: DEAD MA ...\n",
      "singles.txt   25 SEXY MALE, seeks attrac older  ...\n",
      "wine.txt      Lovely delicate, fragrant Rhone w ...\n"
     ]
    }
   ],
   "source": [
    "# import the webtext package from the corpus package of the nltk library\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "# print all the file identifiers of the web text package with a small snipet form the beginning of each text file\n",
    "for fileid in webtext.fileids():\n",
    "    print(f\"{fileid:13} {webtext.raw(fileid)[:33]:33} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also has a corpus of instant messaging chat sessions. The corpus contains over 10,000 posts. The corpus is organized into 15 files, where each file contains several hundred posts collected on a given date, for an age-specific chatroom (teens, 20s, 30s, 40s, plus a generic adults chatroom).\n",
    "\n",
    "The `posts(\"`*`file.xml`*`\")` function of the `corpus.nps_chat` package returns an `XMLCorpusView` from an *xml file*. The returned `XMLCorpusView` contains a list of posts. Each post is a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'do', \"n't\", 'want', 'hot', 'pics', 'of', 'a', 'female', ',', 'I', 'can', 'look', 'in', 'a', 'mirror', '.']\n"
     ]
    }
   ],
   "source": [
    "# import the nps_chat package from the corpus package of the nltk library\n",
    "from nltk.corpus import nps_chat\n",
    "\n",
    "# store the posts from the 10-19-20s_706posts.xml file (20s chat room on 10/19/2006. contains 706 posts)\n",
    "chatroom = nps_chat.posts(\"10-19-20s_706posts.xml\")\n",
    "\n",
    "# print the 123rd post\n",
    "print(chatroom[123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Brown Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK contains The Brown Corpus which contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on.\n",
    "\n",
    "The `categories()` function of the `corpus.brown` package returns a list of all the available categories of The Brown Corpus.\n",
    "\n",
    "The `words(\"`*`fileid`*`\")` function of the `corpus.brown` package returns all the tokens from a file with the specified *file id*. The parameter for the file id can also be a list of file ids. The `words()` function also has a `category` parameter which accepts a category or a list of categories.\n",
    "\n",
    "The `sents(\"`*`fileid`*`\")` function of the `corpus.brown` package returns a list sentences of a file from the Brown Corpus with the specified *file id*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "# import the brown package from the corpus package of the nltk library\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# print the categories of the brown corpus\n",
    "print(brown.categories())\n",
    "\n",
    "# print the tokens from texts of the brown corpus that are categorized as news\n",
    "print(brown.words(categories=\"news\"))\n",
    "\n",
    "# print the tokens from the text of the brown corpus with id \"cg22\"\n",
    "print(brown.words(fileids=[\"cg22\"]))\n",
    "\n",
    "# print the sentences from the texts of the brown corpus that are categorized as news, editorial and reviews\n",
    "print(brown.sents(categories=[\"news\", \"editorial\", \"reviews\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
